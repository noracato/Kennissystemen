{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Nora Schinkel & Maarten Boon\n",
    "\n",
    "__Student id(s)__ : 10416730 & 10764399\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here, ok). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "###  Wordnet (50%, each exercise 10%)\n",
    "\n",
    "* Make Exercises: 5,13,14,26,27 from [WordNet from NLTK](http://www.nltk.org/book/ch02.html#wordnet) NLTK book Chapter 2\n",
    "\n",
    "### DBpedia spotlight (50%)\n",
    "\n",
    "1.  (25%) Create a program (preferably here in this notebook), which can read in a URL, extract the text, run DBPedia spotlight on it, and presents the results in an attractive manner. \n",
    "    * You can use the spotlight API, or run it locally.\n",
    "    * Let the user have some control on the crucial parameters (like confidence, support, etc)\n",
    "    * It should work well on English language text.\n",
    "    * Have some good examples after your code, showing how things work\n",
    "2. (25%) You do a manual evaluation, as is also done in section 4.2 of the spotlight article.\n",
    "    * Manually annotate the following two articles. Indicate the phrases that are named entities, indicate their type, and (if there is any) indicate the wikipedia page in the English Wikipedia which is the best link for that entity.\n",
    "    * Then run DBpedia spotlight on it, and compare the results. Experiment with different settings of the parameters.\n",
    "    * Compute precision and recall for a few different settings, and plot a precison-recall curve (like Figure 3 in the spotlight article). \n",
    "    \n",
    "    * Artciles:\n",
    "        * <http://www.bbc.com/news/world-us-canada-33753067> until \"The deal at a glance\"\n",
    "        * <http://www.bbc.com/news/world-europe-33694773>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My wordnet answers\n",
    "\n",
    "### Q 5\n",
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair.n.01 \n",
      "\t [] \n",
      "\t [Synset('back.n.08'), Synset('leg.n.03')] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "professorship.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "president.n.04 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "electric_chair.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "chair.n.05 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bath.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bath.n.02 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bathtub.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('bathroom.n.01')] \n",
      "\t []\n",
      "bath.n.04 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('homer.n.03')] \n",
      "\t []\n",
      "bath.n.05 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('england.n.01')] \n",
      "\t []\n",
      "bathroom.n.01 \n",
      "\t [] \n",
      "\t [Synset('bathtub.n.01'), Synset('shower_stall.n.01'), Synset('toilet.n.02'), Synset('washbasin.n.02')] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('dwelling.n.01')] \n",
      "\t []\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "baths = wn.synsets(\"bath\", pos=\"n\")\n",
    "chairs = wn.synsets(\"chair\", pos=\"n\")\n",
    "\n",
    "for synset in chairs:\n",
    "   print( synset.name(), \"\\n\\t\", synset.member_meronyms(), \"\\n\\t\", synset.part_meronyms(), \"\\n\\t\", synset.substance_meronyms(),\"\\n\\t\", synset.member_holonyms(),\"\\n\\t\", synset.part_holonyms(),\"\\n\\t\", synset.substance_holonyms())\n",
    "for synset in baths:\n",
    "   print( synset.name(), \"\\n\\t\", synset.member_meronyms(), \"\\n\\t\", synset.part_meronyms(), \"\\n\\t\", synset.substance_meronyms(),\"\\n\\t\", synset.member_holonyms(),\"\\n\\t\", synset.part_holonyms(),\"\\n\\t\", synset.substance_holonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 13 \n",
    "What percentage of noun synsets have no hyponyms? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "nouns = list(wn.all_synsets('n')) #get a list so we can actually get hyponyms\n",
    "from __future__ import division\n",
    "count = len(nouns) #get amount to divide through\n",
    "#print(nouns) #DEBUG\n",
    "nohypo = 0 #clear nohypo\n",
    "for synset in nouns: #for every element of the list\n",
    "    hyps = synset.hyponyms()\n",
    "    if len(hyps) == 0: #always equals 0 for some reason\n",
    "        nohypo += 1   \n",
    "out = nohypo/count * 100 #make output (out = is DEBUG)\n",
    "#print(nohypo) #DEBUG\n",
    "print(out, \"%\") #DEBUG\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 14\n",
    "Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def supergloss(synset):\n",
    "    hyper = ' '.join([hyp.definition() for hyp in synset.hypernyms()])\n",
    "    hypo = ' '.join([hyp.definition() for hyp in synset.hyponyms()])\n",
    "    return (' '.join([synset.definition(), hypo, hyper]))\n",
    "\n",
    "print ((supergloss(wn.synsets(\"oak\")[0])))\n",
    "#' '.join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 26\n",
    "What is the branching factor of the noun hypernym hierarchy? I.e. for every noun synset that has hyponyms — or children in the hypernym hierarchy — how many do they have on average? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all nouns\n",
    "allSets = list(wn.all_synsets('n'))\n",
    "def countsets(synsets):\n",
    "    numSets = len(synsets)\n",
    "    numHyp = 0\n",
    "    # look for amount of hypernyms and hyponyms and add them to numHyp\n",
    "    for sets in synsets:\n",
    "        hypo = sets.hyponyms()\n",
    "        lenHypo = len(hypo)\n",
    "        hyper = sets.hypernyms()\n",
    "        lenHyper = len(hyper)\n",
    "        branches = lenHypo + lenHyper\n",
    "        numHyp += branches\n",
    "    # average is total branches/total nouns\n",
    "    average = numHyp/numSets\n",
    "    return average\n",
    "\n",
    "print(countsets(allSets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q 27\n",
    "The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has 7 senses with: len(wn.synsets('dog', 'n')). Compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "allNouns = list(wn.all_synsets('n'))\n",
    "allVerbs = list(wn.all_synsets('v'))\n",
    "allAdv = list(wn.all_synsets('r'))\n",
    "allAdj = list(wn.all_synsets('a'))\n",
    "\n",
    "def average_senses(type):\n",
    "    allSense = 0\n",
    "    for set in type:\n",
    "        numSense = len(wn.synsets(set)) #werkt niet :(\n",
    "        allSense += numSense\n",
    "    allType = len(type)\n",
    "    average = allSense/allType\n",
    "    return average\n",
    "\n",
    "print(average_senses(allNouns), \"Average senses nouns\")\n",
    "print(average_senses(allVerbs), \"Average senses verbs\")\n",
    "print(average_senses(allAdv), \"Average senses adverbs\")\n",
    "print(average_senses(allAdj), \"Average senses adjectives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Spotlight answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for annotating the text in a URL\n",
    "import requests  # http://docs.python-requests.org/en/latest/\n",
    "\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "url='http://www.bbc.com/news/world-us-canada-33753067'\n",
    "\n",
    "# step 1 get the url and the text out of it\n",
    "bbc= requests.get(url)\n",
    "soup= BeautifulSoup(bbc.text) \n",
    "\n",
    "article=soup.find(\"div\", { \"id\" : \"page\" })  # this expression should be imporved to get just the text from the article\n",
    "\n",
    "article_text= article.text\n",
    "\n",
    "article_text[:2000]\n",
    "\n",
    "#Step 2: Set up the headers and the body of the request to the spotlight API\n",
    "\n",
    "spotlight = \"http://spotlight.dbpedia.org/rest/annotate\"\n",
    "headers = { \"Accept\" : \"application/json\", \"Content-type\" : \"application/x-www-form-urlencoded;charset=utf-8\" }\n",
    "form= { \"text\" : article_text, \"confidence\" : 0.5, \"support\": 20}\n",
    "\n",
    "#Doe de POST request\n",
    "ef= requests.post(spotlight, headers=headers,   data=form)\n",
    "\n",
    "#Pretty parse en print de response van de requestprint(json.dumps(ef.json(), indent=4, separators=( \",\" , \": \"))\n",
    "\n",
    "# code for annotating the text in a URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My annotations of the two articles\n",
    "\n",
    "## Links to annotated HTML/PDFs\n",
    "* Use colored markers to indicate which entities you extracted for linking, using color to indicate their type.\n",
    "\n",
    "## Entities and their links\n",
    "* We  use a similar output format as used by Spotlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for computing precision and recall and making the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
