{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Nora Schinkel & Maarten Boon\n",
    "\n",
    "__Student id(s)__ : 10416730 & 10764399\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here, ok). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>\n",
    "<img src=\"http://i.imgur.com/9g88in2.jpg\" title=\"source:imgur.com\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "###  Wordnet (50%, each exercise 10%)\n",
    "\n",
    "* Make Exercises: 5,13,14,26,27 from [WordNet from NLTK](http://www.nltk.org/book/ch02.html#wordnet) NLTK book Chapter 2\n",
    "\n",
    "### DBpedia spotlight (50%)\n",
    "\n",
    "1.  (25%) Create a program (preferably here in this notebook), which can read in a URL, extract the text, run DBPedia spotlight on it, and presents the results in an attractive manner. \n",
    "    * You can use the spotlight API, or run it locally.\n",
    "    * Let the user have some control on the crucial parameters (like confidence, support, etc)\n",
    "    * It should work well on English language text.\n",
    "    * Have some good examples after your code, showing how things work\n",
    "2. (25%) You do a manual evaluation, as is also done in section 4.2 of the spotlight article.\n",
    "    * Manually annotate the following two articles. Indicate the phrases that are named entities, indicate their type, and (if there is any) indicate the wikipedia page in the English Wikipedia which is the best link for that entity.\n",
    "    * Then run DBpedia spotlight on it, and compare the results. Experiment with different settings of the parameters.\n",
    "    * Compute precision and recall for a few different settings, and plot a precison-recall curve (like Figure 3 in the spotlight article). \n",
    "    \n",
    "    * Artciles:\n",
    "        * <http://www.bbc.com/news/world-us-canada-33753067> until \"The deal at a glance\"\n",
    "        * <http://www.bbc.com/news/world-europe-33694773>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My wordnet answers\n",
    "\n",
    "### Q 5\n",
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair.n.01 \n",
      "\t [] \n",
      "\t [Synset('back.n.08'), Synset('leg.n.03')] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "professorship.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "president.n.04 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "electric_chair.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "chair.n.05 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bath.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bath.n.02 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bathtub.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('bathroom.n.01')] \n",
      "\t []\n",
      "bath.n.04 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('homer.n.03')] \n",
      "\t []\n",
      "bath.n.05 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('england.n.01')] \n",
      "\t []\n",
      "bathroom.n.01 \n",
      "\t [] \n",
      "\t [Synset('bathtub.n.01'), Synset('shower_stall.n.01'), Synset('toilet.n.02'), Synset('washbasin.n.02')] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('dwelling.n.01')] \n",
      "\t []\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "baths = wn.synsets(\"bath\", pos=\"n\")\n",
    "chairs = wn.synsets(\"chair\", pos=\"n\")\n",
    "\n",
    "for synset in chairs:\n",
    "   print( synset.name(), \"\\n\\t\", synset.member_meronyms(), \"\\n\\t\", synset.part_meronyms(), \"\\n\\t\", synset.substance_meronyms(),\"\\n\\t\", synset.member_holonyms(),\"\\n\\t\", synset.part_holonyms(),\"\\n\\t\", synset.substance_holonyms())\n",
    "for synset in baths:\n",
    "   print( synset.name(), \"\\n\\t\", synset.member_meronyms(), \"\\n\\t\", synset.part_meronyms(), \"\\n\\t\", synset.substance_meronyms(),\"\\n\\t\", synset.member_holonyms(),\"\\n\\t\", synset.part_holonyms(),\"\\n\\t\", synset.substance_holonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 13 \n",
    "What percentage of noun synsets have no hyponyms? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "nouns = list(wn.all_synsets('n')) #get a list so we can actually get hyponyms\n",
    "from __future__ import division\n",
    "count = len(nouns) #get amount to divide through\n",
    "#print(nouns) #DEBUG\n",
    "nohypo = 0 #clear nohypo\n",
    "for synset in nouns: #for every element of the list\n",
    "    hyps = synset.hyponyms()\n",
    "    if len(hyps) == 0: #always equals 0 for some reason\n",
    "        nohypo += 1   \n",
    "out = nohypo/count * 100 #make output (out = is DEBUG)\n",
    "#print(nohypo) #DEBUG\n",
    "print(out, \"%\") #DEBUG\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 14\n",
    "Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def supergloss(synset):\n",
    "    hyper = ' '.join([hyp.definition() for hyp in synset.hypernyms()])\n",
    "    hypo = ' '.join([hyp.definition() for hyp in synset.hyponyms()])\n",
    "    return (' '.join([synset.definition(), hypo, hyper]))\n",
    "\n",
    "print ((supergloss(wn.synsets(\"oak\")[0])))\n",
    "#' '.join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 26\n",
    "What is the branching factor of the noun hypernym hierarchy? I.e. for every noun synset that has hyponyms — or children in the hypernym hierarchy — how many do they have on average? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all nouns\n",
    "allSets = list(wn.all_synsets('n'))\n",
    "def countsets(synsets):\n",
    "    numSets = len(synsets)\n",
    "    numHyp = 0\n",
    "    # look for amount of hypernyms and hyponyms and add them to numHyp\n",
    "    for sets in synsets:\n",
    "        hypo = sets.hyponyms()\n",
    "        lenHypo = len(hypo)\n",
    "        hyper = sets.hypernyms()\n",
    "        lenHyper = len(hyper)\n",
    "        branches = lenHypo + lenHyper\n",
    "        numHyp += branches\n",
    "    # average is total branches/total nouns\n",
    "    average = numHyp/numSets\n",
    "    return average\n",
    "\n",
    "print(countsets(allSets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q 27\n",
    "The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has 7 senses with: len(wn.synsets('dog', 'n')). Compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "allNouns = list(wn.all_synsets('n'))\n",
    "allVerbs = list(wn.all_synsets('v'))\n",
    "allAdv = list(wn.all_synsets('r'))\n",
    "allAdj = list(wn.all_synsets('a'))\n",
    "\n",
    "def average_senses(type):\n",
    "    allSense = 0\n",
    "    for set in type:\n",
    "        numSense = len(wn.synsets(set)) #werkt niet :(\n",
    "        allSense += numSense\n",
    "    allType = len(type)\n",
    "    average = allSense/allType\n",
    "    return average\n",
    "\n",
    "print(average_senses(allNouns), \"Average senses nouns\")\n",
    "print(average_senses(allVerbs), \"Average senses verbs\")\n",
    "print(average_senses(allAdv), \"Average senses adverbs\")\n",
    "print(average_senses(allAdj), \"Average senses adjectives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Spotlight answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US President  as word number  1  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "Barack Obama  as word number  14  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Barack_Obama\n",
      "climate change  as word number  121  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate_change\n",
      "Clean  as word number  159  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Clean_Air_Act\n",
      "Power  as word number  165  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_%28physics%29\n",
      "Plan  as word number  171  in text is annotated to link: \n",
      " http://dbpedia.org/resource/University_of_Texas_at_Austin\n",
      "greenhouse gas emissions  as word number  186  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Greenhouse_gas\n",
      "power stations  as word number  219  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_station\n",
      "wind  as word number  316  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Wind_power\n",
      "solar power  as word number  325  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Solar_power\n",
      "renewable energy sources  as word number  347  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Renewable_energy\n",
      "energy industry  as word number  398  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Energy_industry\n",
      "planet  as word number  518  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Planet\n",
      "Obama  as word number  530  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Barack_Obama\n",
      "climate change  as word number  645  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate_change\n",
      "Clean  as word number  674  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Clean_Air_Act\n",
      "Power  as word number  680  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_%28physics%29\n",
      "Plan  as word number  686  in text is annotated to link: \n",
      " http://dbpedia.org/resource/University_of_Texas_at_Austin\n",
      "reduce  as word number  709  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Redox\n",
      "CO2 emissions  as word number  716  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Greenhouse_gas\n",
      "Obama administration  as word number  833  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Presidency_of_Barack_Obama\n",
      "carbon  as word number  866  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Carbon_dioxide_equivalent\n",
      "pollution  as word number  873  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Pollution\n",
      "reduction  as word number  883  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Redox\n",
      "power plants  as word number  903  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_station\n",
      "Coal mining  as word number  1076  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Coal_mining\n",
      "Wyoming  as word number  1103  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Wyoming\n",
      "West Virginia  as word number  1112  in text is annotated to link: \n",
      " http://dbpedia.org/resource/West_Virginia\n",
      "Kentucky  as word number  1130  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Kentucky\n",
      "economies  as word number  1150  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Economy\n",
      "President Obama  as word number  1205  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Presidency_of_Barack_Obama\n",
      "major  as word number  1296  in text is annotated to link: \n",
      " http://dbpedia.org/resource/John_Major\n",
      "climate  as word number  1302  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate\n",
      "summit  as word number  1310  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Summit_%28meeting%29\n",
      "Paris  as word number  1320  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Paris\n",
      "countries  as word number  1358  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Country\n",
      "power  as word number  1433  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_%28physics%29\n",
      "emission  as word number  1471  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Emissions_trading\n",
      "Environmental Protection Agency  as word number  1542  in text is annotated to link: \n",
      " http://dbpedia.org/resource/United_States_Environmental_Protection_Agency\n",
      "BBC's  as word number  1609  in text is annotated to link: \n",
      " http://dbpedia.org/resource/BBC\n",
      "Washington  as word number  1630  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Washington,_D.C.\n",
      "President Obama  as word number  1646  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Presidency_of_Barack_Obama\n",
      "climate change  as word number  1721  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate_change\n",
      "president  as word number  1789  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "authority  as word number  1809  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Authority\n",
      "global  as word number  1841  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Global_warming\n",
      "greenhouse gases  as word number  1862  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Greenhouse_gas\n",
      "major  as word number  1884  in text is annotated to link: \n",
      " http://dbpedia.org/resource/John_Major\n",
      "Paris  as word number  1904  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Paris\n",
      "year  as word number  1921  in text is annotated to link: \n",
      " http://dbpedia.org/resource/University\n",
      "governors  as word number  1949  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Governor\n",
      "White House  as word number  2042  in text is annotated to link: \n",
      " http://dbpedia.org/resource/White_House\n",
      "climate  as word number  2120  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate\n",
      "president  as word number  2141  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "cabinet  as word number  2159  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Cabinet_%28government%29\n",
      "Hillary Clinton  as word number  2167  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Hillary_Rodham_Clinton\n",
      "White House  as word number  2213  in text is annotated to link: \n",
      " http://dbpedia.org/resource/White_House\n",
      "Obama  as word number  2229  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Barack_Obama\n",
      "health  as word number  2369  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Health\n",
      "Climate change  as word number  2386  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate_change\n",
      "Obama  as word number  2460  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Barack_Obama\n",
      "America  as word number  2524  in text is annotated to link: \n",
      " http://dbpedia.org/resource/United_States\n",
      "Clean  as word number  2534  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Clean_Air_Act\n",
      "Power  as word number  2540  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_%28physics%29\n",
      "Plan  as word number  2546  in text is annotated to link: \n",
      " http://dbpedia.org/resource/University_of_Texas_at_Austin\n",
      "climate change  as word number  2614  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate_change\n",
      "Democratic  as word number  2632  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Democratic_Party_%28United_States%29\n",
      "presidential candidate  as word number  2643  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President\n",
      "Hillary Clinton  as word number  2666  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Hillary_Rodham_Clinton\n",
      "Obama  as word number  2746  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Barack_Obama\n",
      "Republican  as word number  2785  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Republican_Party_%28United_States%29\n",
      "Republican  as word number  2838  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Republican_Party_%28United_States%29\n",
      "candidate  as word number  2849  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Candidate\n",
      "president  as word number  2863  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "Republican  as word number  2961  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Republican_Party_%28United_States%29\n",
      "presidential candidate  as word number  2972  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President\n",
      "Marco Rubio  as word number  2996  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Marco_Rubio\n",
      "Florida  as word number  3070  in text is annotated to link: \n",
      " http://dbpedia.org/resource/University_of_Florida\n",
      "governor  as word number  3078  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Governor_of_California\n",
      "Jeb Bush  as word number  3087  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Jeb_Bush\n",
      "Supreme Court  as word number  3154  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Supreme_Court_of_the_United_States\n",
      "EPA  as word number  3200  in text is annotated to link: \n",
      " http://dbpedia.org/resource/United_States_Environmental_Protection_Agency\n",
      "authority  as word number  3212  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Authority\n",
      "greenhouse gas emissions  as word number  3234  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Greenhouse_gas\n",
      "regulation  as word number  3263  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Regulation\n",
      "Republican  as word number  3332  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Republican_Party_%28United_States%29\n",
      "leadership  as word number  3343  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Leadership\n",
      "science  as word number  3450  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Science\n",
      "climate  as word number  3529  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate\n",
      "energy  as word number  3541  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Energy_development\n",
      "president  as word number  3563  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "architect  as word number  3583  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Architect\n",
      "BBC's  as word number  3614  in text is annotated to link: \n",
      " http://dbpedia.org/resource/BBC\n",
      "election  as word number  3668  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Election\n",
      "Democrats  as word number  3693  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Democratic_Party_%28United_States%29\n",
      "policy  as word number  3731  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Policy\n",
      "renewable energy sources  as word number  3814  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Renewable_energy\n",
      "coal  as word number  3944  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Coal\n",
      "power  as word number  3955  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_%28physics%29\n",
      "natural gas  as word number  3964  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Natural_gas\n",
      "carbon dioxide  as word number  4000  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Carbon_dioxide\n",
      "natural gas  as word number  4077  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Natural_gas\n",
      "power generation  as word number  4095  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Electricity_generation\n",
      "current  as word number  4115  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Electric_current\n",
      "Power stations  as word number  4130  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_station\n",
      "greenhouse gases  as word number  4171  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Greenhouse_gas\n",
      "environment  as word number  4281  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Environmentalism\n",
      "president  as word number  4331  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "history  as word number  4430  in text is annotated to link: \n",
      " http://dbpedia.org/resource/History\n",
      "White House  as word number  4442  in text is annotated to link: \n",
      " http://dbpedia.org/resource/White_House\n",
      "power  as word number  4483  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_%28physics%29\n",
      "Republican  as word number  4581  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Republican_Party_%28United_States%29\n",
      "Washington  as word number  4620  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Washington,_D.C.\n",
      "president  as word number  4670  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "EPA  as word number  4741  in text is annotated to link: \n",
      " http://dbpedia.org/resource/United_States_Environmental_Protection_Agency\n",
      "carbon emissions  as word number  4757  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Greenhouse_gas\n",
      "Clean Air Act  as word number  4784  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Clean_Air_Act\n",
      "regulations  as word number  4957  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Regulation\n",
      "Republican  as word number  4980  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Republican_Party_%28United_States%29\n",
      "governors  as word number  4991  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Governor\n",
      "president  as word number  5050  in text is annotated to link: \n",
      " http://dbpedia.org/resource/President_of_the_United_States\n",
      "global  as word number  5121  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Global_warming\n",
      "treaty  as word number  5128  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Treaty\n",
      "climate change  as word number  5138  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate_change\n",
      "Paris  as word number  5156  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Paris\n",
      "year  as word number  5180  in text is annotated to link: \n",
      " http://dbpedia.org/resource/University\n",
      "capital  as word number  5310  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Capitalism\n",
      "climate change  as word number  5370  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Climate_change\n",
      "Clean  as word number  5456  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Clean_Air_Act\n",
      "Power  as word number  5462  in text is annotated to link: \n",
      " http://dbpedia.org/resource/Power_%28physics%29\n",
      "Plan  as word number  5468  in text is annotated to link: \n",
      " http://dbpedia.org/resource/University_of_Texas_at_Austin\n"
     ]
    }
   ],
   "source": [
    "# code for annotating the text in a URL\n",
    "import requests  # http://docs.python-requests.org/en/latest/\n",
    "\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "myUrl='http://www.bbc.com/news/world-us-canada-33753067'\n",
    "\n",
    "\n",
    "def annotate(url, confidence, support):\n",
    "    # step 1 get the url and the text out of it\n",
    "    bbc= requests.get(url)\n",
    "    soup= BeautifulSoup(bbc.text) \n",
    "\n",
    "    article=soup.find(\"div\", { \"class\" : \"story-body__inner\" })  # this expression should be imporved to get just the text from the article\n",
    "    article_text= article.text\n",
    "    article_text[:2000]\n",
    "\n",
    "    better_text = re.split(r'\\n{2,}', article_text) #Split in paragraphs\n",
    "    filter_text = ' '.join([paragraphs for paragraphs in better_text if not paragraphs.startswith(' ')]) #remove all indented paragraphs (javascript)\n",
    "\n",
    "    #Step 2: Set up the headers and the body of the request to the spotlight API\n",
    "\n",
    "    spotlight = \"http://spotlight.dbpedia.org/rest/annotate\"\n",
    "    headers = { \"Accept\" : \"application/json\", \"Content-type\" : \"application/x-www-form-urlencoded;charset=utf-8\" }\n",
    "    form= { \"text\" : filter_text, \"confidence\" : confidence, \"support\": support}\n",
    "\n",
    "    #Doe de POST request\n",
    "    ef= requests.post(spotlight, headers=headers,   data=form)\n",
    "\n",
    "    #Pretty parse en print de response van de requestprint(json.dumps(ef.json(), indent=4, separators=( \",\" , \": \"))\n",
    "\n",
    "    basicAn_text = ef.json()\n",
    "    all_annotations = basicAn_text[\"Resources\"]\n",
    "    for annotation in all_annotations:\n",
    "        surface_form = annotation[\"@surfaceForm\"]\n",
    "        offset = annotation[\"@offset\"]\n",
    "        uri = annotation[\"@URI\"]\n",
    "        print(surface_form, \" as word number \", offset, \" in text is annotated to link: \\n\", uri)\n",
    "        \n",
    "\n",
    "annotate(myUrl, 0.2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My annotations of the two articles\n",
    "\n",
    "## Links to annotated HTML/PDFs\n",
    "* Use colored markers to indicate which entities you extracted for linking, using color to indicate their type.\n",
    "\n",
    "### Article 1\n",
    "Used green for 'eigennamen' and pink for the rest.\n",
    "https://github.com/noracato/Kennissystemen/blob/master/Annotate.pdf\n",
    "\n",
    "In total there are 29 pink and 19 green annotations listed at:\n",
    "https://github.com/noracato/Kennissystemen/blob/master/Annotationstxt1.txt\n",
    "\n",
    "\n",
    "\n",
    "### Article 2\n",
    "Maarten vult hier zn links in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities and their links\n",
    "* We  use a similar output format as used by Spotlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# all urls\n",
    "url1 = 'http://www.bbc.com/news/world-us-canada-33753067'\n",
    "url1b = 'https://github.com/noracato/Kennissystemen/blob/master/Annotationstxt1.txt'\n",
    "\n",
    "url2 = 'http://www.bbc.com/news/world-europe-33694773'\n",
    "url2b = '' #Maarten vul hier je git link in!\n",
    "   \n",
    "# strip the wanted elements of text on a site (url) from the site, type refers to type of site    \n",
    "def strip_text(url, type):\n",
    "    bbc= requests.get(url)\n",
    "    soup= BeautifulSoup(bbc.text)\n",
    "    \n",
    "    # get the wanted text from a site, we have two different sites: bbc and the github site\n",
    "    if type == 'bbc':\n",
    "        article=soup.find(\"div\", { \"class\" : \"story-body__inner\" }) #bbc documents\n",
    "    else:\n",
    "        article=soup.find(\"div\", { \"class\" : \"blob-wrapper data type-text\" }) # txt files on github   \n",
    "    article_text= article.text\n",
    "    article_text[:2000]\n",
    "\n",
    "    # handle article text and manual(github)text differently \n",
    "    if type == 'bbc':\n",
    "        better_text = re.split(r'\\n{2,}', article_text) #Split in paragraphs\n",
    "        filter_text = ' '.join([paragraphs for paragraphs in better_text if not paragraphs.startswith(' ')]) #remove all indented paragraphs (javascript)\n",
    "    else:\n",
    "        better_text = re.split(r'\\n', article_text) #Split in parts\n",
    "        filter_text = [lines for lines in better_text if not lines.startswith('http')] #remove all wikilinks\n",
    "        filter_text = [lines for lines in filter_text if lines] #remove all empty lines\n",
    "    \n",
    "    return filter_text\n",
    "\n",
    "def get_annotated_words(url, confidence, support, type):\n",
    "    text = strip_text(url, type)\n",
    "    \n",
    "    # setup the spotlight\n",
    "    spotlight = \"http://spotlight.dbpedia.org/rest/annotate\"\n",
    "    headers = { \"Accept\" : \"application/json\", \"Content-type\" : \"application/x-www-form-urlencoded;charset=utf-8\" }\n",
    "    form= { \"text\" : text, \"confidence\" : confidence, \"support\": support}\n",
    "    ef= requests.post(spotlight, headers=headers,   data=form)\n",
    "    \n",
    "    # run the spotlight and return all annotations made\n",
    "    basicAn_text = ef.json()\n",
    "    all_annotations = basicAn_text[\"Resources\"]\n",
    "    list_annotated_words = [annotation[\"@surfaceForm\"] for annotation in all_annotations]\n",
    "    \n",
    "    return list_annotated_words\n",
    "\n",
    "# get annotations with different parameters and create lists per article\n",
    "def get_an_1():\n",
    "    annotations_code_article1_0 = get_annotated_words(url1, 0.3, 20, 'bbc')\n",
    "    annotations_code_article1_1 = get_annotated_words(url1, 0.3, 80, 'bbc')\n",
    "    annotations_code_article1_2 = get_annotated_words(url1, 0.3, 230, 'bbc')\n",
    "    annotations_code_article1_3 = get_annotated_words(url1, 0.3, 1000, 'bbc')\n",
    "    annotations_code_article1 =[annotations_code_article1_0, annotations_code_article1_1, annotations_code_article1_2, annotations_code_article1_3]\n",
    "    return annotations_code_article1\n",
    "\n",
    "annotations_man_article1 = strip_text(url1b, 'git')\n",
    "\n",
    "def get_an_2():\n",
    "    annotations_code_article2_0 = get_annotated_words(url2, 0.3, 20, 'bbc')\n",
    "    annotations_code_article2_1 = get_annotated_words(url2, 0.3, 80, 'bbc')\n",
    "    annotations_code_article2_2 = get_annotated_words(url2, 0.3, 230, 'bbc')\n",
    "    annotations_code_article2_3 = get_annotated_words(url2, 0.3, 1000, 'bbc')\n",
    "    annotations_code_article2 =[annotations_code_article2_0, annotations_code_article2_1, annotations_code_article2_2, annotations_code_article2_3]\n",
    "    return annotations_code_article2\n",
    "\n",
    "#annotations_man_article2 = strip_text(url2b, 'git')\n",
    "# Maarten haalt hier de hashtag boven weg\n",
    "\n",
    "print(\"done\") #check if list is made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# simple function to calculate recall\n",
    "def recall(output, true):\n",
    "    same = [term for term in output if term in true]   #vergelijk zelfde woorden in beide lijsten\n",
    "    recall = len(same)/len(true) #zelfde woorden/alle goede woorden\n",
    "    return recall\n",
    "    \n",
    "# simple function to calculate precision   \n",
    "def precision(output, true):\n",
    "    same = [term for term in output if term in true] #vergelijk zelfde woorden in beide lijsten\n",
    "    precision = len(same)/len(output) #zelfde woorden/ output woorden\n",
    "    return precision\n",
    "\n",
    "def plot_an(article):\n",
    "    # get all recall and precision values and plot\n",
    "    if article == 1:\n",
    "        recall1 = [recall(annotations, annotations_man_article1) for annotations in annotations_code_article1]\n",
    "        precision1 = [precision(annotations, annotations_man_article1) for annotations in annotations_code_article1]\n",
    "        plt.plot(recall1, precision1, 'bo')\n",
    "    elif article == 2:\n",
    "        recall2 = [recall(annotations, annotations_man_article2) for annotations in annotations_code_article2]\n",
    "        precision2 = [precision(annotations, annotations_man_article2) for annotations in annotations_code_article2]\n",
    "        plt.plot(recall2, precision2, 'ro')\n",
    "    #else do both articles\n",
    "    else:\n",
    "        recall1 = [recall(annotations, annotations_man_article1) for annotations in annotations_code_article1]\n",
    "        precision1 = [precision(annotations, annotations_man_article1) for annotations in annotations_code_article1]\n",
    "        plt.plot(recall1, precision1, 'bo')\n",
    "        recall2 = [recall(annotations, annotations_man_article2) for annotations in annotations_code_article2]\n",
    "        precision2 = [precision(annotations, annotations_man_article2) for annotations in annotations_code_article2]\n",
    "        plt.plot(recall2, precision2, 'ro')\n",
    "    plt.show()\n",
    "\n",
    "plot_an(1)\n",
    "#plot_an(0)\n",
    "\n",
    "#Maarten wissel deze om :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
