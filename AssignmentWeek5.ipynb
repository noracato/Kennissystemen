{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  week 5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Nora Schinkel & Maarten Boon\n",
    "\n",
    "__Student id(s)__ : 10416730 & 10764399\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here, ok). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "###  Wordnet (50%, each exercise 10%)\n",
    "\n",
    "* Make Exercises: 5,13,14,26,27 from [WordNet from NLTK](http://www.nltk.org/book/ch02.html#wordnet) NLTK book Chapter 2\n",
    "\n",
    "### DBpedia spotlight (50%)\n",
    "\n",
    "1.  (25%) Create a program (preferably here in this notebook), which can read in a URL, extract the text, run DBPedia spotlight on it, and presents the results in an attractive manner. \n",
    "    * You can use the spotlight API, or run it locally.\n",
    "    * Let the user have some control on the crucial parameters (like confidence, support, etc)\n",
    "    * It should work well on English language text.\n",
    "    * Have some good examples after your code, showing how things work\n",
    "2. (25%) You do a manual evaluation, as is also done in section 4.2 of the spotlight article.\n",
    "    * Manually annotate the following two articles. Indicate the phrases that are named entities, indicate their type, and (if there is any) indicate the wikipedia page in the English Wikipedia which is the best link for that entity.\n",
    "    * Then run DBpedia spotlight on it, and compare the results. Experiment with different settings of the parameters.\n",
    "    * Compute precision and recall for a few different settings, and plot a precison-recall curve (like Figure 3 in the spotlight article). \n",
    "    \n",
    "    * Artciles:\n",
    "        * <http://www.bbc.com/news/world-us-canada-33753067> until \"The deal at a glance\"\n",
    "        * <http://www.bbc.com/news/world-europe-33694773>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My wordnet answers\n",
    "\n",
    "### Q 5\n",
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair.n.01 \n",
      "\t [] \n",
      "\t [Synset('back.n.08'), Synset('leg.n.03')] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "professorship.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "president.n.04 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "electric_chair.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "chair.n.05 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bath.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bath.n.02 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t []\n",
      "bathtub.n.01 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('bathroom.n.01')] \n",
      "\t []\n",
      "bath.n.04 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('homer.n.03')] \n",
      "\t []\n",
      "bath.n.05 \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('england.n.01')] \n",
      "\t []\n",
      "bathroom.n.01 \n",
      "\t [] \n",
      "\t [Synset('bathtub.n.01'), Synset('shower_stall.n.01'), Synset('toilet.n.02'), Synset('washbasin.n.02')] \n",
      "\t [] \n",
      "\t [] \n",
      "\t [Synset('dwelling.n.01')] \n",
      "\t []\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "baths = wn.synsets(\"bath\", pos=\"n\")\n",
    "chairs = wn.synsets(\"chair\", pos=\"n\")\n",
    "\n",
    "for synset in chairs:\n",
    "   print( synset.name(), \"\\n\\t\", synset.member_meronyms(), \"\\n\\t\", synset.part_meronyms(), \"\\n\\t\", synset.substance_meronyms(),\"\\n\\t\", synset.member_holonyms(),\"\\n\\t\", synset.part_holonyms(),\"\\n\\t\", synset.substance_holonyms())\n",
    "for synset in baths:\n",
    "   print( synset.name(), \"\\n\\t\", synset.member_meronyms(), \"\\n\\t\", synset.part_meronyms(), \"\\n\\t\", synset.substance_meronyms(),\"\\n\\t\", synset.member_holonyms(),\"\\n\\t\", synset.part_holonyms(),\"\\n\\t\", synset.substance_holonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 13 \n",
    "What percentage of noun synsets have no hyponyms? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 14\n",
    "Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'hypernyms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fa9e0442b00a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypernymDef\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msupergloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oak\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-fa9e0442b00a>\u001b[0m in \u001b[0;36msupergloss\u001b[0;34m(synset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msupergloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdefinition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mhypernymDef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhyps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypernymDef\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'hypernyms'"
     ]
    }
   ],
   "source": [
    "def supergloss(synset):\n",
    "    definition = [sets.definition() for sets in synset]\n",
    "    #hyps = synset.hypernyms()\n",
    "    #hypernymDef = [hyp.definition() for hyp in hyps]\n",
    "    return definition\n",
    "\n",
    "print (supergloss(wn.synsets(\"oak\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 26\n",
    "What is the branching factor of the noun hypernym hierarchy? I.e. for every noun synset that has hyponyms — or children in the hypernym hierarchy — how many do they have on average? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q 27\n",
    "The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has 7 senses with: len(wn.synsets('dog', 'n')). Compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Spotlight answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for annotating the text in a URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My annotations of the two articles\n",
    "\n",
    "## Links to annotated HTML/PDFs\n",
    "* Use colored markers to indicate which entities you extracted for linking, using color to indicate their type.\n",
    "\n",
    "## Entities and their links\n",
    "* We  use a similar output format as used by Spotlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for computing precision and recall and making the plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
